{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the training of the model and will give the final training, validation and test error for the model. To satisfy every requirements to run the notebook, you can follow `Evaluate-the-final-model` in the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorboard as tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to import that evaluations as a dataframe, we used the tensorboard dev api. The results of our evaluation can be seen here [Tensorboard.dev](https://tensorboard.dev/experiment/dTD0vaI3SdyRZYI4WLHRbg/#scalars). The following cell collects that data and turns it into a pandas dataframe with the run as the index and the losses + performance metrics in the columns.\n",
    "\n",
    "The documentation for this process can be found [here](https://www.tensorflow.org/tensorboard/dataframe_api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the id in the URL of the tensorboard.dev webpage\n",
    "experiment_id = \"dTD0vaI3SdyRZYI4WLHRbg\" \n",
    "# Download the data from the tensorboard project\n",
    "experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "# Get the scalars into a dataframe\n",
    "df = experiment.get_scalars()\n",
    "# reshape it to have the run as the rows\n",
    "output  = df.pivot(index=\"run\", columns=\"tag\", values=\"value\")\n",
    "# Drop the eval row of the dataframe, this is a residual folder kept in tensorboard memory\n",
    "# which contains the same information as the last evaluation performed (on test set here)\n",
    "output = output.drop('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tag</th>\n",
       "      <th>DetectionBoxes_Precision/mAP</th>\n",
       "      <th>DetectionBoxes_Precision/mAP (large)</th>\n",
       "      <th>DetectionBoxes_Precision/mAP (medium)</th>\n",
       "      <th>DetectionBoxes_Precision/mAP (small)</th>\n",
       "      <th>DetectionBoxes_Precision/mAP@.50IOU</th>\n",
       "      <th>DetectionBoxes_Precision/mAP@.75IOU</th>\n",
       "      <th>DetectionBoxes_Recall/AR@1</th>\n",
       "      <th>DetectionBoxes_Recall/AR@10</th>\n",
       "      <th>DetectionBoxes_Recall/AR@100</th>\n",
       "      <th>DetectionBoxes_Recall/AR@100 (large)</th>\n",
       "      <th>DetectionBoxes_Recall/AR@100 (medium)</th>\n",
       "      <th>DetectionBoxes_Recall/AR@100 (small)</th>\n",
       "      <th>Loss/classification_loss</th>\n",
       "      <th>Loss/localization_loss</th>\n",
       "      <th>Loss/regularization_loss</th>\n",
       "      <th>Loss/total_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.577580</td>\n",
       "      <td>0.804416</td>\n",
       "      <td>0.420162</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>0.823290</td>\n",
       "      <td>0.649656</td>\n",
       "      <td>0.369214</td>\n",
       "      <td>0.635027</td>\n",
       "      <td>0.658216</td>\n",
       "      <td>0.844016</td>\n",
       "      <td>0.557507</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.223413</td>\n",
       "      <td>0.003314</td>\n",
       "      <td>0.037377</td>\n",
       "      <td>0.264104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>0.600287</td>\n",
       "      <td>0.818538</td>\n",
       "      <td>0.414392</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>0.840275</td>\n",
       "      <td>0.676805</td>\n",
       "      <td>0.393724</td>\n",
       "      <td>0.652730</td>\n",
       "      <td>0.672312</td>\n",
       "      <td>0.853080</td>\n",
       "      <td>0.558387</td>\n",
       "      <td>0.147729</td>\n",
       "      <td>0.181020</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.037377</td>\n",
       "      <td>0.221419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>0.582718</td>\n",
       "      <td>0.802349</td>\n",
       "      <td>0.398904</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.826129</td>\n",
       "      <td>0.655632</td>\n",
       "      <td>0.381782</td>\n",
       "      <td>0.640276</td>\n",
       "      <td>0.662531</td>\n",
       "      <td>0.840806</td>\n",
       "      <td>0.551840</td>\n",
       "      <td>0.156835</td>\n",
       "      <td>0.197278</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.037377</td>\n",
       "      <td>0.237886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tag         DetectionBoxes_Precision/mAP  \\\n",
       "run                                        \n",
       "test                            0.577580   \n",
       "training                        0.600287   \n",
       "validation                      0.582718   \n",
       "\n",
       "tag         DetectionBoxes_Precision/mAP (large)  \\\n",
       "run                                                \n",
       "test                                    0.804416   \n",
       "training                                0.818538   \n",
       "validation                              0.802349   \n",
       "\n",
       "tag         DetectionBoxes_Precision/mAP (medium)  \\\n",
       "run                                                 \n",
       "test                                     0.420162   \n",
       "training                                 0.414392   \n",
       "validation                               0.398904   \n",
       "\n",
       "tag         DetectionBoxes_Precision/mAP (small)  \\\n",
       "run                                                \n",
       "test                                    0.004719   \n",
       "training                                0.005160   \n",
       "validation                              0.007373   \n",
       "\n",
       "tag         DetectionBoxes_Precision/mAP@.50IOU  \\\n",
       "run                                               \n",
       "test                                   0.823290   \n",
       "training                               0.840275   \n",
       "validation                             0.826129   \n",
       "\n",
       "tag         DetectionBoxes_Precision/mAP@.75IOU  DetectionBoxes_Recall/AR@1  \\\n",
       "run                                                                           \n",
       "test                                   0.649656                    0.369214   \n",
       "training                               0.676805                    0.393724   \n",
       "validation                             0.655632                    0.381782   \n",
       "\n",
       "tag         DetectionBoxes_Recall/AR@10  DetectionBoxes_Recall/AR@100  \\\n",
       "run                                                                     \n",
       "test                           0.635027                      0.658216   \n",
       "training                       0.652730                      0.672312   \n",
       "validation                     0.640276                      0.662531   \n",
       "\n",
       "tag         DetectionBoxes_Recall/AR@100 (large)  \\\n",
       "run                                                \n",
       "test                                    0.844016   \n",
       "training                                0.853080   \n",
       "validation                              0.840806   \n",
       "\n",
       "tag         DetectionBoxes_Recall/AR@100 (medium)  \\\n",
       "run                                                 \n",
       "test                                     0.557507   \n",
       "training                                 0.558387   \n",
       "validation                               0.551840   \n",
       "\n",
       "tag         DetectionBoxes_Recall/AR@100 (small)  Loss/classification_loss  \\\n",
       "run                                                                          \n",
       "test                                    0.157143                  0.223413   \n",
       "training                                0.147729                  0.181020   \n",
       "validation                              0.156835                  0.197278   \n",
       "\n",
       "tag         Loss/localization_loss  Loss/regularization_loss  Loss/total_loss  \n",
       "run                                                                            \n",
       "test                      0.003314                  0.037377         0.264104  \n",
       "training                  0.003023                  0.037377         0.221419  \n",
       "validation                0.003231                  0.037377         0.237886  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing to note here is that the cost evaluated during the training was calculated on a subset of the whole training data set while this training error was calculated on the entire train set. Despite this the values are very similiar.\n",
    "\n",
    "In general, the mAP and AR, as well as the total loss are all relatively close together when evaluated on each of the three sets. The total loss had the highest change between training and test, most likely due to it being the combination of the classification, localization and regularization loss. It is also not surprising to have a constant regularization loss as this loss depends on the weights of the network (which are the same for any used set).\n",
    "\n",
    "If we focus on the total loss, we get these 3 values:\n",
    "- test loss: 0.264104\n",
    "- train loss: 0.221419\n",
    "- validation loss: 0.237886\n",
    "\n",
    "The validation and train loss are pretty close. The test loss is a bit higher (0.04 higher than the train loss). Even though this difference is very small, we could think that it is due to overfitting. According to the loss curves presented in `Monitoring-the-loss` in the README.md, the evaluation on validation set was very close to the training set during the whole training, which is not an indication of overfitting. So the fitting graph is not showing a sign of overfitting. In addition, the validation set have a loss close to the training set. As the validation set has never been used to update the weights during training (the gradients are not computed during evaluation), this shows that the model generalized well, without overfitting the data. If we focus more on the resuts of the evaluation on test set ,we notice that this difference of loss is mainly due to the classification loss which is 0.04 higher than the train set. Thus, this difference can be due to the data in the test set which can be slightly harder to classify, with harder examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
